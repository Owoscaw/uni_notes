
Let $T:\Re^n\mapsto\Re^n$ be a [[Linear Maps#Properties of linear maps|linear mapping]] from [[Real vector spaces|$\Re^n$]] to $\Re^n$. $T$ is represented as an $n\times n$ [[Matrix definition|matrix]] $M$. The form of $M$ depends on the [[Bases and dimensions in RN|basis]], and changes under a change of basis. Suppose $\underline x\in\Re^n$ is given wrt the [[Vector space definitions#Zero vector and Standard Basis vectors|standard basis]] as $x_1\underline e_1+\dots+x_n\underline e_n$ and that $M$ is the matrix representing $T$ in this basis:$$\Huge T(\underline x)=M\begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix}$$If $\{\underline v_1,\dots,\underline v_n\}$ is a different basis, then $\underline x=\tilde x_1\underline v_1+\dots+\tilde x_n\underline v_n$. That is to say:$$\Huge \begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix}=P\begin{pmatrix}\tilde x_1\\\vdots\\\tilde x_n\end{pmatrix}$$Then the matrix $N$ representing the same linear map $T$ wrt the second basis is given by:$$\Huge T(\underline x)=N\begin{pmatrix}\tilde x_1\\\vdots\\\tilde x_n\end{pmatrix}\implies N=P^{-1}MP$$Loosely speaking, $P^{-1}$ changes the tilda'd coordinates to normal coordinates, $M$ changes according to the linear map, then $P$ changes the normal coordinates to the tilda'd coordinates. This is useful in, for example, the predator prey model where the population of a predator and prey after $n$ years, $\underline v_n$ is in the form $\underline v_n=M^nv_0$ where $M$ represents the change in population year on year and $v_0$ is the initial population. If $M$ were diagonal, then it is trivial to say that:$$\Huge M=\begin{pmatrix}a&0\\0&b\end{pmatrix}\implies M^n=\begin{pmatrix}a^n&0\\0&b^n\end{pmatrix}$$ To do this, it is crucial to diagonalise $M$.

A non-zero vector $v$ is an eigenvector of a matrix $A$ with eigenvalue $\lambda\in\mathbb C$ if:$$\Huge A\underline v=\lambda\underline v\implies(A-I\lambda)\underline v=\underline 0$$If $v$ is an eigenvector, so is $\mu\underline v$ with the same eigenvalue $\lambda$ for any $\mu\neq0$. Since we require $\underline v\neq0$, then $A-I\lambda=\underline 0=B$. $B$ is not invertible as if it were, then $B\underline v=\underline 0\implies B^{-1}B\underline v=B^{-1}\underline 0=\underline 0\implies \underline v=\underline 0$, which is a contradiction. So we require that $\det B=0$:$$\Huge \det{B}=\det{(A-\lambda I)}=0$$
# Characteristic polynomials:

Notice that $P_A(t)=\det(A-tI)$, where $A$ is an $n\times n$ matrix, is a polynomial of degree at most $n$ in $t$, the characteristic polynomial. So the following implication is made:$$\Huge \lambda\text{  is an eigenvalue of }A\implies\lambda\text{  is a root of }P_A(t)$$If $t^\prime$ is a root of $P_A(t)$ then $P_A(t^\prime)=0\implies \det(A-t^\prime I)=0\implies A-t^\prime I$ is not invertible, so there is at least one $\underline v\neq\underline 0$ such that $(A-t^\prime I)\underline v=\underline 0$. So we have that $A\underline v=t^\prime\underline v$, which implies $t^\prime$ is an eigenvalue of $A$:$$\Huge \lambda\text{  is a root of }P_A(t)\implies\lambda\text{  is an eigenvalue of }A$$This completes both sides of the implication, so:$$\Huge \lambda\text{  is an eigenvalue of }A\iff\lambda\text{  is a root of }P_A(t)$$

## Multiplicity:

f a polynomial $p(t)$ has degree $n$, then:$$\Huge p(t)=a(t-\lambda_1)^{\kappa_1}(t-\lambda_2)^{\kappa_2}\dots(t-\lambda_p)^{\kappa_p}$$With $a\neq0$ and $\lambda_1,\dots,\lambda_p$ distinct complex numbers, which are the roots of $p$, $p(\lambda_i)=0$. For $\kappa_i\in\mathbb N$ restricted to $1\leq\kappa_i\leq n$:$$\Huge \sum_{i=1}^p\kappa_i=n$$
Appling this to the characteristic polynomial, we get:$$\Huge P_A(t)=a(t-\lambda_1)^{\kappa_1}\dots(t-\lambda_p)^{\kappa_p}$$So we say that the algebraic multiplicity of the eigenvalue $\lambda_i$ is $\kappa_i$, as this root is repeated $\kappa_i$ times in the polynomial. Given $\lambda$ is an eigenvalue with algebraic multiplicity $\kappa_\lambda$ then:$$\Huge P_A(t)=(t-\lambda)^{\kappa_\lambda}Q(t)$$Where $Q(t)\neq0$ and has degree $n-\kappa_\lambda$.

Suppose that there are $p_\lambda$ linearly independent eigenvectors of $A$ with eigenvalues $\lambda$ where $1\leq p_\lambda\leq\kappa_\lambda$, then $p_\lambda$ is called the geometric multiplicity of $\lambda$. To find eigenvectors, find $P_A(t)$ and solve for roots $\lambda$, then find $\underline v\neq\underline 0$ such that $A\underline v=\lambda\underline v$. There is at least one eigenvector for all matrices $A$. Since we're solving for $\underline v\neq\underline 0$ such that $B\underline v=\underline 0$, $\underline v$ will always be in the kernel of $B=A-\lambda I$.

# Eigenspaces:

The eigenspace $V_\lambda$ for the eigenvalue $\lambda$ is the $p_\lambda$-dimensional [[Subspaces#Spanning sets|vector subspace]] spanned by these eigenvectors. That is:$$\Huge V_\lambda=\ker(A-\lambda I)$$This is because:$$\large \underline v\in V_\lambda=\ker(A-\lambda I)\implies (A-\lambda I)\underline v=0\implies A\underline v-\lambda(I\underline v)=0\implies A\underline v=\lambda\underline v$$Since there is always at least one and at most $\kappa_\lambda$:$$\Huge 1\leq \dim V_\lambda=\dim\ker(A-\lambda I)\leq\kappa_\lambda=\text{algebraic multiplicity}$$In fact $\dim V_{\lambda}=p_\lambda=\text{geometric multiplicity}=null(A-\lambda I)$.  

# Cayley-Hamilton theorem:

Let $A\in M_{n\times n}(\Re)$ with a characteristic polynomial $p_A(t)$. Then $p_A(A)=0$. 

Let matrices $A,B$ represent the same linear transformation $T$ but in different bases. Then $A$ and $B$ should be considered equivalent, as they will have the same eigenvalues however will be different matrices.
## Equivalence relation:
An equivalence relation on the set $X$ is a binary relation "$\sim$" with properties (for $a,b,c\in X$):
>$a\sim a$, reflexivity
>$a\sim b\implies b\sim a$, symmetry
>$a\sim b$, $b\sim c\implies a\sim c$, transitivity

Then we can say that for $A,B\in M_n(\Re)$, $A$ and $B$ are similar ($A\sim B$) if $\exists M:A=M_{-1}BM$:$$\Huge A\sim B\iff\exists M:A=M_{-1}BM$$We then propose that similarity is an equivalence relation:![[equiv relation similar]]
So the set of all $n\times n$ matrices gets partitioned into equivalence classes, that is classes of matrices that are similar:$$\Huge [A]=\{B\in M_{n}(\Re):B\sim A\}$$$[A]$ will contain all matrices that represent the same linear transformation in all possible bases. We propose that similar matrices have the same eigenvalues and characteristic polynomials, proven as follows:![[similar matrices same polynomial proof]]
# Diagonalisation:

$A$ is said to be diagonalisable if $A\sim D$ with $D$ being any diagonal matrix. That is $\exists M:M^{-1}AM=D$. The following are a summary of facts about diagonal and similar matrices:
> For two diagonalisable matrices that have the same eigenvalues, $A$ and $B$, then $A\sim B$. Since these matrices are diagonalisable, we have that $A\sim D_1$ and $B\sim D_2$ where $D_1$ and $D_2$ are diagonal matrices. However since $A$ and $B$ share eigenvalues, $D_1\sim D_2$ and since similarity is transitive, we get that $A\sim B$.
> If $A$ is diagonalisable and $B$ is not, then they cannot be similar. Since $A$ is diagonalisable, $A\sim D$. However assuming $A\sim B$, then by transitivity of similarity we have that $B\sim D$, contradicting the assumption.
> Not all square matrices are diagonalisable.
> If all eigenvalues of a matrix $A$ are distinct, then $A$ is diagonalisable.
> If $A$ is real and symmetric then $A$ is diagonalisable.

The matrix $A\in M_n(\Re)$ is diagonalisable if and only if it has exactly $n$ linearly independent eigenvalues. Proof: Suppose that $\{v_1,v_2,\dots,v_n\}$ is a set of linearly independent eigenvectors with associated eigenvalues $\{\lambda_1,\lambda_2,\dots,\lambda_n\}$. From this assemble the matrix:$$\Huge M=\begin{pmatrix}v_1&v_2&\dots&v_n\end{pmatrix}$$And let:$$\Huge D=\begin{pmatrix}\lambda_1&0&\dots&0\\0&\lambda_2&\dots&0\\\vdots&\vdots&\ddots&\vdots\\0&0&\dots&\lambda_n\end{pmatrix}$$Observe that:$$\Huge AM=\begin{pmatrix}\lambda_1v_1&\dots&\lambda_nv_n\end{pmatrix}=\begin{pmatrix}v_1&\dots&v_n\end{pmatrix}\begin{pmatrix}\lambda_1&0&\dots&0\\0&\lambda_2&\dots&0\\\vdots&\vdots&\ddots&\vdots\\0&0&\dots&\lambda_n\end{pmatrix}=MD$$So we can say that $M^{-1}AM=D$, which implies that $A$ is diagonalisable since $A\sim D$ and $D$ is diagonal. Therefore, if $A\in M_n(\Re)$ has $n$ linearly independent eigen vectors, it is implied that it is diagonalisable. For the other direction of implication, we know that $A\sim D$ so $\exists M$ such that $M^{-1}AM=D$. This implies $AM=MD$, so the columns of $M$ are the eigenvectors of $A$, forming a linearly independent set since $M$ was invertible. Therefore if $A$ has $n$ linearly independent eigenvectors, it is implied to be in $M_n(\Re)$. This concludes the proof. Examples:![[diagonalising examples]]
Note that eigenvalues may not necessarily be distinct, take the characteristic polynomial:$$\Huge p_A(t)=a(t-\lambda_1)^{k_1}(t-\lambda_2)^{k_2}\dots(t-\lambda_p)^{k^p}$$Where $a\neq0$ and $\lambda_i\neq\lambda_j$ for $i\neq j$, eigenvalues are distinct. Each $\lambda_i$ has algebraic multiplicity $k_i$. We also get that for the matrix $A\in M_n(\Re)$:$$\Huge \sum_{i=1}^pk_i=n$$The eigenspace is formed from all possible eigenvalues:$$\Huge V_\lambda=\ker(A-\lambda I)=\{\underline v:(A-\lambda I)\underline v=\underline 0\}$$Which gives the inequality:$$\Huge 1\leq\dim V_\lambda\leq k_\lambda$$From this we propose that eigenvectors corresponding to distinct eigenvalues must be linearly independent. We prove this for a pair of eigenvectors, which can be taken over pairwise eigenvectors for any amount of eigenvectors:![[distinct eigenvectors are lin ind proof]]This is generalised to $n$ distinct eigenvectors. This has a corollary where for a matrix $A\in M_n(\Re)$ with $n$ distinct eigenvalues, then $A$ is diagonalisable.