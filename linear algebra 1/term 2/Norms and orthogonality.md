Given a [[Vector space definitions|vector space]] over $\Re$, a norm on $V$ is a function $||\cdot||:V\mapsto\Re$ such that:
> $||a\underline v||=|a|||\underline v||$, absolute homogeneity
> $||\underline u+\underline v||\leq||\underline u||+||\underline v||$, triangle inequality
> $||\underline v||=0\iff\underline v=\underline0$, separation of points

Using absolute homogeneity, observe that $||-\underline v||=||\underline v||$, making the triangle inequality:$$\Huge ||\underline v+(-\underline v)||=||0\cdot\underline v||=0\leq||\underline v||+||-\underline v||=2||\underline v||$$So we get that $||\underline v||\geq0$, non-negativity. When $V$ has an [[Inner product spaces|inner product]], there is a special norm induced by the inner product. In the real case, let $\{V,(\cdot,\cdot)\}$ be a real inner product space, then we let the norm induced by $(\cdot,\cdot)$ be defined by:$$\Huge ||\underline v||=\sqrt{(\underline v,\underline v)}\in\Re^+$$This is well defined since $(\underline v,\underline v)\geq0$ by definition. We define the unit vector if $||\underline v||=1$, and orthogonality if $(\underline u,\underline v)=0$, denoted as $\underline u\perp\underline v$.

We propose that if $\underline u\perp\underline v$ then:$$\Huge ||\underline u+\underline v||^2=||\underline u||^2+||\underline v||^2$$To prove this consider $||\underline u+\underline v||^2=(\underline u+\underline v,\underline u+\underline v)=(\underline u,\underline u)+2(\underline u,\underline v)+(\underline v,\underline v)=||\underline u||^2+0+0+||\underline v||^2$. Note also $||a\underline v||=\sqrt{(a\underline v,a\underline v)}=\sqrt{a^2(\underline v,\underline v)}=|a|||\underline v||$. So the definition of the norm induced by $(\cdot,\cdot)$ agrees with the definition of the norm.

# Cauchy-Schwarz inequality:

For a real inner product space $\{V,(\cdot,\cdot)\}$:$$\Huge (\underline u,\underline v)^2\leq(\underline u,\underline u)(\underline v,\underline v)$$The inequality becomes equality when $\underline u,\underline v$ are linearly dependent. We prove the above by first considering $\underline v=0$, in this case the proof is trivial so take $\underline v\neq0$ without loss of generality. Consider $\underline w=\underline u-\frac{(\underline u,\underline v)}{||\underline v||^2}\underline v$, then:$$0\leq||\underline w||^2=(\underline w,\underline w)=\left(\underline u-\frac{(\underline u,\underline v)}{||\underline v||^2}\underline v,\underline u-\frac{(\underline u,\underline v)}{||\underline v||^2}\underline v\right)=(\underline u,\underline u)-2\frac{(\underline u,\underline v)^2}{||\underline v||^2}+\frac{(\underline u,\underline v)^2(\underline v,\underline v)}{||\underline v||^2||\underline v||^2}=(\underline u,\underline u)-\frac{(\underline u,\underline v)^2}{(\underline v,\underline v)}$$This implies the inequality by multiplying through by $(\underline v,\underline v)$ and adding $(\underline u,\underline v)^2$ to the other side of the inequality. We also use this to prove the triangle inequality as follows:
$$ (||\underline u||+||\underline v||)^2-||\underline u+\underline v||^2=||\underline u||^2+2||\underline u||\times||\underline v||+||\underline v||^2-2(\underline u,\underline v)-||\underline v||^2=2(||\underline u||\times||\underline v||-(\underline u,\underline v))\geq0$$$$\Huge \implies(||\underline u||+||\underline v||)^2\geq||\underline u+\underline v||^2\implies||\underline u||+||\underline v||\geq||\underline u+\underline v||$$We can then define the angle between two vectors. We suppose that the angle, $\theta\in[0,\pi]$ is given by:$$\Huge (\underline u,\underline v)=||\underline u||\times||\underline v||\times\cos\theta$$
# [[Inner product spaces#Complex inner products|Hermitian]] norms:

Let $\{V,\langle,\rangle\}$ be a complex inner product space. We then define the norm induced by $\langle,\rangle$:||v―||=⟨v―,v―⟩For all $v\in V$. Orthogonality and unit vectors are then defined in the same way.

## Complex Cauchy-Schwarz inequality:
We suppose:|⟨u―,v―⟩|2≤||u―||2||v―||2To prove this, assume $\underline u\neq0$ and $\langle\underline u,\underline v\rangle\neq0$ and consider:||xu―−v―||2=|x|2||u―||2−2ℜ(x⟨u―,v―⟩)+||v―||2≥0Since this must be non-negative for all $x\in\mathbb C$ we choose:x=λ|⟨u―,v―⟩|⟨u―,v―⟩For some $\lambda\in\Re$, the equation becomes:λ2||u―||2−2λ|⟨u―,v―⟩|+||v―||2≥0At this point, the same argument used for the real Cauchy-Schwarz inequality can be used to prove the statement. The triangle inequality for a Hermitian inner product can also be proven using the same method as before, however taking advantage of the fact $\langle\underline u,\underline v\rangle+\langle\underline v,\underline u\rangle=2\Re(\langle\underline u,\underline v\rangle)$ by hermiticity. Note that the angle between vectors in a Hermitian inner product space is not well defined, however orthogonality is still well defined, so we define the orthonormal basis:

## Orthonormal basis:
An orthonormal basis $\{\underline u_1,\dots,\underline u_n\}$ of $V$ is a basis consisting of mutually orthogonal normal vectors, we can write this condition as:$$\Huge \langle\underline u_i,\underline u_j\rangle=\delta_{i,j}$$

# Gram-Schmidt procedure:

The Gram-Schmidt procedure is a systematic method to produce an orthonormal basis for a given inner product space, from a set of linearly independent vectors. Let $\underline v_1,\dots,\underline v_k$ be $k$ linearly independent vectors in an inner product space $V$, these will span a $k$-dimensional subspace $U\subset V$. We aim to construct an orthonormal basis $\underline u_1,\dots,\underline u_k$ of $U$:
> Define $\underline u_1=\frac{\underline v_1}{||\underline v_1||}$, then we have that $\underline u_1$ is a unit vector and $span(\underline u_1)=span(\underline v_1)$
> Define $\underline{\bar v_2}=\underline v_2-(\underline v_2,\underline u_1)\underline u_1$. Then $(\underline u_1,\underline{\bar v_2})=0$ with $\underline{\bar v_2}\neq\underline 0$ since $\underline v_2\notin span(\underline u_1)$. Now define $\underline u_2=\frac{\underline{\bar v_2}}{||\underline{\bar v_2}||}$, then both $\underline u_1$ and $\underline u_2$ are both unit vectors and are mutually orthogonal with $span\{\underline u_1,\underline u_2\}=span\{\underline v_1,\underline v_2\}$
> Suppose now that we find mutually orthogonal unit vectors $\underline u_1,\dots,\underline u_r$ with $span\{\underline u_1,\dots,\underline u_r\}=span\{\underline v_1,\dots,\underline v_r\}$ for some $1\leq r\leq k$. If $r<k$ then define:$$\large \underline{\bar v}_{r+1}=\underline v_{r+1}-(\underline v_{r+1},\underline u_1)\underline u_1-\dots-(\underline v_{r+1},\underline u_r)\underline u_r\implies(\underline{\bar v}_{r+1},\underline u_1)=\dots=(\underline{\bar v}_{r+1},\underline u_r)=0$$We know $\underline{\bar v}_{r+1}\neq\underline 0$ since $\underline{\bar v}_{r+1}\notin span\{\underline u_1,\dots,\underline u_r\}=span\{\underline v_1,\dots,\underline v_r\}$. Define $\underline u_{r+1}=\frac{\underline{\bar v}_{r+1}}{||\underline{\bar v}_{r+1}||}$, then we have that $\underline u_1,\dots,\underline u_{r+1}$ are mutually orthogonal unit vectors with $span\{\underline u_1,\dots,\underline u_{r+1}\}=span\{\underline v_1,\dots,\underline v_{r+1}\}$

By this inductive process, we can construct an orthonormal basis for $U$. Note if $U=V$, we can use this process to create an orthonormal basis for $V$. Also not that $U=span\{\underline u_1,\dots,\underline u_n\}$ and $\dim U=k$ 

# Orthogonal complement and projection:

Let $U$ be a [[Subspaces|vector subspace]] of the inner product space $V$, then the orthogonal complement of $U$ is defined as:$$\Huge U^\perp=\{\underline v\in V:(\underline u,\underline v)=0\,\forall u\in U\}$$This is the set of all vectors in $V$ that are orthogonal to all the vectors in $U$.

Given any vector $\underline v\in V$, there is a unique vector $\underline u\in U$ called the orthogonal projection of $\underline v$ onto $U$, said $\underline u$ can be found as follows:

Propose that if $U$ is finite dimensional, then there always exists a unique decomposition $\underline v=\underline u+\underline{\bar u}$ where $\underline u\in U,\underline{\bar u}\in U^\perp$, so we write: $$\Huge V=U\oplus U^\perp$$Let $\{\underline u_1,\dots,\underline u_k\}$ be an orthonormal basis of $U$ and set:$$\Huge \underline u=(\underline v,\underline u_1)\underline u_1+\dots+(\underline v,\underline u_k)\underline u_k,\,\,\underline{\bar u}=\underline v-\underline u$$Clearly $\underline u\in U$ as it is a linear combination of each $\underline u_i$, and for each $\underline u_i$ note that $(\underline u,\underline u_i)=(\underline v,\underline u_i)\times(\underline u_i,\underline u_i)=(\underline v,\underline u_i)$, so for each $\underline u_i$ we have:$$\Huge (\underline{\bar u},\underline u_i)=(\underline v-\underline u,\underline u_i)=(\underline v,\underline u_i)-(\underline u,\underline u_i)=(\underline v,\underline u_i)-(\underline v,\underline u_i)=0$$Therefore $\underline u_i$ and $\underline{\bar u}$ are orthogonal and $\underline{\bar u}\in U^\perp$ 

Given a finite dimensional vector subspace $U$ of $\{V,(\cdot,\cdot)\}$, the orthonormal projection operator $P$ onto $U$ is defined as:$$\Huge P_U(\underline v)=(\underline v,\underline u_1)\underline u_1+\dots+(\underline v,\underline u_k)\underline u_k$$Where $k=\dim U$ and $\{\underline u_1,\dots,\underline u_k\}$ is an orthonormal basis of $U$. This operator has the following properties:
> $P_U$ is linear, $P_U:V\mapsto V$
> $Im\,P_U=U$ and $\ker P_U=U^\perp$ 
> $P_U$ is idempotent, $P_U^2=P_U$
> $P(\underline u)=\underline u$ for all $\underline u\in U$, that is $P_U|_U=I$
> $P_U(1-P_U)=(1-P_U)P_U=0$ 

We propose that if $U$ is a finite dimensional subspace of $\{V,(\cdot,\cdot)\}$, $\underline v_0\in V$, $\underline u_o\in P_U(\underline v_0)$, then we have that:$$\Huge ||\underline u-\underline v_0||\geq||\underline u_0-\underline v_0||,\,\,\forall \underline u\in U$$With equality if and only if $\underline u=\underline u_0$:![[Norms and orthogonality .excalidraw]]Notice the right angle triangles. We set $\underline{\bar u}_0=\underline v_0-\underline v_0$, then $\underline{\bar u}_0\in U^\perp$ and use pythagoras:$$\Huge ||\underline v_0-\underline u||^2=||\underline v_0-\underline u_0+\underline u_0-\underline u||=||\underline v_0-\underline u_0||^2+||\underline u_0-\underline u||^2\geq||\underline v_0-\underline u_0||^2$$Since $||\underline v_0-\underline u_0||=||\underline u_0-\underline v_0||$, we then take square roots and get the inequality as required.

## Bessel's Inequality:

Let $\{V,(\cdot,\cdot)\}$ be an inner product space, and $U$ a finite dimension subspace. If $v\in V$ and $u\in P_U(\underline v)$, then $||\underline u||^2\leq||\underline v||^2$. In particular if $\{u_1,\dots,u_k\}$ is an orthonormal basis for $U$ and $\underline u=\lambda_1\underline u_1+\dots+\lambda_k \underline u_k$ then:$$\Huge \sum_{i=1}^k\lambda_i^2\leq||\underline v||^2$$To prove this, we write $\underline v=\underline u+\underline{\bar u}$ and show:$$\Huge \sum_{i=1}^k \lambda_i^2=||\underline u||^2\leq||\underline u||^2+||\underline{\bar u}||^2=||\underline v||^2$$The fact that the LHS sum is given by $||\underline u||^2=(\underline u,\underline u)$ follows from the bilinearity of the inner product together with orthogonality of the $\{u_1,\dots,u_k\}$ basis.

# Orthogonality and Unitary diagonalisation:

A real $n\times n$ matrix is orthogonal if:$$\Huge M^TM=MM^T=I$$That is $M^T=M^{-1}$. The set of $n\times n$ orthogonal matrices is called the orthogonal group and is denoted by:$$\Huge O(n)=\{M\in GL(n,\Re):M^TM=MM^T=I\}$$Since $\det M=\det M^T$, orthogonal matrices must have $\det M=\pm 1$. The subset of all orthogonal matrices with determinant $+1$ is known as the special orthogonal group, denoted by:$$\Huge SO(n)=\{M\in O(n):\det M=\pm1\}$$
A complex $n\times n$ matrix is called unitary if:$$\Huge UU^*=U^*U=I$$That is $U^*=U^{-1}$. Then the set of $n\times n$ unitary matrices is known as the unitary group, denoted by:$$\Huge U(n)=\{U\in GL(n,\mathbb{C}):U^*U=UU^*=I\}$$Similarly, the subset of all orthogonal matrices with determinant $+1$ is known as the special unitary group, denoted by:$$\Huge SU(n)=\{U\in U(n):\det U=1\}$$Here we use $+1$ because $\det U^*=\bar{\det U}$, so any unitary matrix has $|\det U|^2=\det U\det U^*=\det(UU^*)=1$. Note that a matrix is orthogonal or unitary if and only if its columns form an orthonormal basis for the standard linear inner product on $\Re^n$ or $\mathbb{C}^n$.

We propose that if $A$ is complex Hermitian or real symmetric, then the eigenvalues of $A$ are real. Suppose that $A \underline x=\lambda \underline x$ with $\underline x\neq0$, then $\underline x^*A \underline x=\lambda \underline x^* \underline x$. Taking the conjugate transpose of this expression gives $\underline x^*A^*\underline x=\bar \lambda \underline x^* \underline x$. Now since $A=A^*$ we see that $\underline x^*A \underline x=\bar \lambda \underline x^* \underline x=\lambda \underline x^* \underline x$, by our first equation. So we gat that $\lambda=\bar \lambda$, therefore $\Im(\lambda)=0$, making $\lambda$ real. Note that if $A$ is real symmetric then also the eigenvectors can be taken as real, since they satisfy $(A-\lambda I)\underline v=\underline 0$ with $A\in M_n(\Re)$ and $\det(A-\lambda I)=0$.

Let $A$ be complex Hermitian or real symmetric, then the eigenvectors of $A$ corresponding to distinct eigenvalues are mutually orthogonal with the standard inner product on $\Re^n$ or $\mathbb{C}^n$. Suppose $\lambda,\mu$ are eigenvalues of $A$, both are real by the above proof. We then have $A \underline x=\lambda \underline x,A \underline y=\mu \underline y$ with $\underline x,\underline y\neq \underline 0$. Therefore $\underline x^*A \underline y=\mu \underline x^* \underline y$ and $\underline y^*A \underline x=\lambda \underline y^* \underline x$, then $\underline x^*A^* \underline y=\bar \lambda \underline x^* \underline y=\underline x^*A \underline y$ since we have $A^*=A$. Now this is the same as the first equation, so they are equivalent. Since $\lambda\in\Re$ we have $\mu \underline x^*\underline y=\lambda \underline x^*\underline y$, which implies $(\lambda-\mu)\underline x^*\underline y=0$, which implies $\underline x^*\underline y=0$, since $\lambda\neq \mu$. Therefore $\underline x\perp \underline y$, since this will correspond to the standard inner product being equal to $0$.

If all eigenvalues of $A$ are distinct, this is sufficient to say that there exists an orthonormal eigenvector basis for $\Re^n$ or $\mathbb{C}^n$ for $A$. In fact even is some eigenvalues are repeated, $A$ can still be diagonalised. We propose that if $A$ is real symmetric, then there is an $n\times n$ orthogonal matrix $P$ such that $P^TAP=D$ where $D$ is a diagonal matrix. If $A$ is complex Hermitian, then there is an $n\times n$ unitary matrix $P$ such that $P^*AP=D$ where $D$ is diagonal and real.

In each case above, the columns of $P$ are an orthonormal basis of eigenvectors of $A$. We focus on the complex case: Choose an eigenvalue $\lambda_1$ and associated eigenvector $\underline u_1$ so that we have $A \underline u_1=\lambda_1 \underline u_1$ and write $U_1=span\{\underline u_1\}$ to denote the eigenspace spanned by $\underline u_1$. We then construct $U_1^\perp$, the orthogonal complement of $\underline u_1$. Using the standard inner product on $\mathbb{C}^n$, we write:$$\Huge \mathbb{C}^n=U_1\oplus U_1^\perp$$Now note that $A$ acting on any element  $v\in U_1^\perp$ gives another element in $U_1^\perp$:$$\Huge \langle A \underline v,\underline u_1\rangle=(\underline u_1)^*A \underline v=(A \underline u_1)^*\underline v=\bar \lambda_1 \underline u_1^*\underline v=\bar \lambda_1\langle \underline v, \underline u_1\rangle=0$$Using the hermiticity of $A$. We denote this restriction as $A|_{U_1^\perp}:U_1^\perp\mapsto U_1^\perp$, this is a Hermititan operator acting on $U_1^\perp\cong \mathbb{C}^{n-1}$. Now this process can be repeated for the other eigenvalues of $A$. Now pick another eigenvalue $\lambda_2$ of $A|_{U_1^\perp}$ with an associated eigenvector $\underline u_2$ and define $U_2=span\{\underline u_2\}$   