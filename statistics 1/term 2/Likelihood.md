
The likelihood is critical for both Frequentist and Bayesian statistics. We imagine data is gathered $\{X_1,\dots,X_n\}$. The precise distribution of $X$ may not be known, but we may suspect that it belongs to a family of possible distributions, with possible parameters $\underline \theta$. For example a [[Random sampling and proportions#Bernoulli distribution|Bernoulli distribution]] satisfies $\theta=\{p\}$, and a [[Random sampling and proportions#Normal approximation|normal distribution]] satisfies $\theta=\{\mu,\sigma\}$. The joint PDF of the data is given by $f(x_1,\dots,x_n|\underline \theta)$. However once data is observed, the PDF can be thought of as a function of $\underline \theta$ as each variable is now fixed.

# The likelihood function:

Suppose $X_1,\dots,X_n$ has a joint distribution $f(x_1,\dots,x_n|\underline \theta)$. The likelihood function $L(\theta)$ is the PDF of the observed data as a function of $\theta$:$$\Huge L(\theta)=L(\theta:x_1,\dots,x_n)=f(x_1,\dots,x_n|\theta)$$We then define the log likelihood function as it's logarithm:$$\Huge \mathcal{L}(\theta)=\mathcal{L}(\theta:x_1,\dots, x_n)=\log(L(\theta))=\log f(x_1,\dots,x_n|\theta)$$The likelihood for identically distributed, independent data where each $X_i$ has PDF $f(x|\theta)$ is:$$\Huge L(\theta)=f(x_1,\dots,x_n|\theta)=\prod_{i=1}^nf(x_i|\theta)$$This makes the log likelihood function:$$\Huge \mathcal{L}(\theta)=\log\left(\prod_{i=1}^nf(x_i|\theta)\right)=\sum_{i=1}^nf(x_i|\theta)$$Note that $L(\theta)$ is not a PDF with respect to $\theta$, so it does not integrate to $1$. This is very useful as it can be used to determine what $\theta$ could have generated the $X_i$ data.$$\Huge \text{Gay Porn}$$