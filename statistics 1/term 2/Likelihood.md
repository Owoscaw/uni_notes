
The likelihood is critical for both Frequentist and Bayesian statistics. We imagine data is gathered $\{X_1,\dots,X_n\}$. The precise distribution of $X$ may not be known, but we may suspect that it belongs to a family of possible distributions, with possible parameters $\underline \theta$. For example a [[Random sampling and proportions#Bernoulli distribution|Bernoulli distribution]] satisfies $\theta=\{p\}$, and a [[Random sampling and proportions#Normal approximation|normal distribution]] satisfies $\theta=\{\mu,\sigma\}$. The joint PDF of the data is given by $f(x_1,\dots,x_n|\underline \theta)$. However once data is observed, the PDF can be thought of as a function of $\underline \theta$ as each variable is now fixed.

# The likelihood function:

Suppose $X_1,\dots,X_n$ has a joint distribution $f(x_1,\dots,x_n|\underline \theta)$. The likelihood function $L(\theta)$ is the PDF of the observed data as a function of $\theta$:$$\Huge L(\theta)=L(\theta:x_1,\dots,x_n)=f(x_1,\dots,x_n|\theta)$$We then define the log likelihood function as it's logarithm:$$\Huge \mathcal{L}(\theta)=\mathcal{L}(\theta:x_1,\dots, x_n)=\log(L(\theta))=\log f(x_1,\dots,x_n|\theta)$$The likelihood for identically distributed, independent data where each $X_i$ has PDF $f(x|\theta)$ is:$$\Huge L(\theta)=f(x_1,\dots,x_n|\theta)=\prod_{i=1}^nf(x_i|\theta)$$This makes the log likelihood function:$$\Huge \mathcal{L}(\theta)=\log\left(\prod_{i=1}^nf(x_i|\theta)\right)=\sum_{i=1}^n\log(f(x_i|\theta))$$Note that $L(\theta)$ is not a PDF with respect to $\theta$, so it does not integrate to $1$. This is very useful as it can be used to determine what $\theta$ could have generated the $X_i$ data. The Poisson distribution is often used to model radioactive decay counts. If data is drawn from a Poisson distribution, we say that $X_i\sim Po(\lambda)$ where $\lambda$ is the parameter of interest. Note that $E[X_i]=\lambda$ and $f(x|\lambda)=\frac{e^{-\lambda}\lambda^x}{x!}$. If we have IID data $X_i$, then the likelihood is:$$\Huge L(\lambda)=f(x_1,\dots,x_n|\lambda)=\prod_{i=0}^nf(x_i|\lambda)=\prod_{i=0}^n\frac{e^{-\lambda}\lambda^{x_i}}{x_i!}=\frac{e^{-n \lambda}\lambda^{\sum_{i=0}^nx_i}}{\prod_{i=0}^nx_i!}$$$$\large \mathcal{L}(\lambda)=\log(f(x_1,\dots,x_n|\lambda))=\log\left(\frac{e^{-n \lambda}\lambda^{\sum_{i=0}^nx_i}}{\prod_{i=0}^nx_i!}\right)=-n \lambda+\sum_{i=0}^nx_i\log(\lambda)-\log\left(\prod_{i=0}^m x_i!\right)$$
# Maximum likelihood estimation:

This can give us a very general procedure to estimate unknown parameters. We aim to find the value of $\theta$ that maximises the likelihood function $L(\theta)$, we denote such parameter as $\hat \theta_{MLE}$:$$\Huge \hat \theta_{MLE}=\arg_{\theta}\max L(\theta)=\arg_\theta\max \mathcal{L}(\theta)$$This is because $\log$ is monotonic increasing, maximising $\mathcal{L}$ is a lot easier in general. For general data $x_i$, the general form of $\hat \theta_{MLE}$ can be found, so we consider it a full estimator. In general, MLEs have good properties, especially for large $n$. However they are not always unbiased. Take our Poisson example  as above. We aim to maximise the expression $\mathcal{L}(\lambda)$:$$\Huge \frac{\partial \mathcal{L}}{\partial \lambda}=-n+\frac{1}{\lambda}\sum_{i=0}^nx_i=0\implies\hat \lambda_{MLE}=\frac{1}{n}\sum_{i=0}^nx_i=\bar x$$Before data is measured, the MLE can be written as:$$ \hat \lambda_{MLE}=\frac{1}{n}\sum_{i=0}^nX_i=\bar X\implies E[\hat \lambda_{MLE}]=\frac{1}{n}E\left(\sum_{i=0}^nX_i\right)=\frac{1}{n}\sum_{i=0}^nE[X_i]=\frac{1}{n}\sum_{i=0}^n \lambda=\frac{1}{n}\times n \lambda= \lambda$$So $\hat \lambda_{MLE}$ is indeed unbiased

# Sufficiency via Factorisation Theorem:

If we have a sample of data $x_1,\dots,x_n$, a statistic $T(x_1,\dots,x_n)$ is said to be sufficient for $\theta$ if and only if we can factorise $L(\theta)$ as:$$\Huge L(\theta)=(x_1,\dots,x_n|\theta)=g(T,\theta)h(x_1,\dots,x_n)$$Where $g$ depends only on the statistic and the parameter, which contains the information about the shape of $L(\theta)$, whereas $h$ is a multiplicative constant that contains details about the exact data used. A sufficient statistic $T$ will contain all the information about $\theta$ from the data. This definition can be generalised for a collection of statistics by considering vectors. If $T$ is a sufficient statistic for $\theta$, we can partially differentiate $L(\theta)$ to find $\hat \theta_{MLE}$:$$\large \frac{\partial L}{\partial \theta}=\frac{d}{d \theta}(g(T,\theta)h(x_1,\dots,x_n))=h(x_1,\dots,x_n)\frac{d}{d \theta}g(T,\theta)=0\implies \frac{d}{d \theta}g(T, \theta)=0$$So for $\hat \theta_{MLE}$ to maximise $L(\theta)$, it must satisfy the above equation and must only be a function of $T$, not the data. Take the Poission distribution as an example: Suppose we have $X_1,\dots,X_n$ independent $Po(\lambda)$ observations:$$\large L(\lambda)=f(x_1,\dots,x_n|\lambda)=\prod_{i=1}^nf(x_i|\lambda)=\prod_{i=1}^n\frac{e^{-\lambda}\lambda^{x_i}}{x_i!}=\frac{e^{-n \lambda}\prod_{i=1}^n\lambda^{x_i}}{\prod_{i=1}^nx_i!}=\frac{e^{-n \lambda}\lambda^{n\bar x}}{\prod_{i=1}^nx_i!}$$Where $\bar x$ is the sample mean. We then apply the factorisation theorem:$$\Huge L(\lambda)=(e^{-n \lambda}\lambda^{n\bar x})\left(\frac{1}{\prod_{i=1}^nx_i!}\right)$$Here, $g(T,\theta)$ is of the form $g(\bar x,\lambda)$, so $T=\bar x$ is a sufficient statistic for $\lambda$, that is $\hat \lambda_{MLE}=\bar x$. Note that a sufficient statistic is not necesarrily unique.